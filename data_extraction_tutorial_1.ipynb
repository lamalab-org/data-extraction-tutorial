{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial: Large Language Models for scientific data extraction\n",
    "\n",
    "Structured data is at the heart of machine learning. LLMs offer a convenient way to generate structured data based on unstructured inputs. This tutorial gives hands-on examples of the different steps in the extraction workflow using LLMs.\n",
    "\n",
    "More extensive and detailed examples can be found at [Matextract.pub](https://matextract.pub)\n",
    "\n",
    "\n",
    " **Tutorial Instructions**\n",
    "\n",
    "This notebook is designed as a hands-on exercise.  \n",
    "Selected parts of the code are intentionally left incomplete and marked with `___` to encourage active participation.  \n",
    "You do not need to download any external files or documents — all example data is already included and ready to use.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c36e04f642335768"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8de262072c63ca49"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Obtaining data\n",
    "\n",
    "At the start of the data extraction process you have to collect a set of potentially relevant data sources. To do so, you could collect a dataset manually or use a tool to help to automate and speed up this process. One of these libraries is crossrefapi.\n",
    "\n",
    "Besides the API there are multiple Python libraries available that make access to the API easier. One of these libraries is crossrefapi. As an example, 10 sources including metadata on the topic ‘copolymerization’ are extracted and saved into a JSON file."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "728a11ac0dfc38c8"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'DOI': '10.1021/jacs.4c05094.s001', 'issued': {'date-parts': [[None]]}, 'publisher': 'American Chemical Society (ACS)', 'title': ['Radical Stitching Polymerization and Its Alternating Copolymerization'], 'type': 'component'}, {'DOI': '10.1016/0032-3950(62)90580-4', 'issued': {'date-parts': [[1962, 1]]}, 'publisher': 'Elsevier BV', 'title': ['Studies in cyclic polymerization and copolymerization—V. Cyclic copolymerization of divinylacetals with vinyl acetate'], 'type': 'journal-article'}, {'DOI': '10.1021/acs.biomac.7b00229.s002', 'issued': {'date-parts': [[None]]}, 'publisher': 'American Chemical Society (ACS)', 'title': ['Enzymatically Debranched Xylans in Graft Copolymerization'], 'type': 'component'}, {'DOI': '10.1021/acsmacrolett.7b00904.s002', 'issued': {'date-parts': [[None]]}, 'publisher': 'American Chemical Society (ACS)', 'title': ['Nickel-Catalyzed Propylene/Polar Monomer Copolymerization'], 'type': 'component'}, {'DOI': '10.1021/acs.macromol.8b00696.s001', 'issued': {'date-parts': [[None]]}, 'publisher': 'American Chemical Society (ACS)', 'title': ['CO2 Controlled Catalysis: Switchable Homopolymerization and Copolymerization'], 'type': 'component'}, {'DOI': '10.1021/jacs.9b02316.s001', 'issued': {'date-parts': [[None]]}, 'publisher': 'American Chemical Society (ACS)', 'title': ['Alternating Copolymerization of Inorganic Nanoparticles'], 'type': 'component'}, {'DOI': '10.1021/bk-1982-0187.ch018', 'author': [{'given': 'RUBIE', 'family': 'CHEN', 'sequence': 'first', 'affiliation': [{'name': 'Centre de Recherche en Pâtes et Papiers, Université du Québec à Trois-Rivières, Trois-Rivières, Québec, Canada G9A 5H7'}]}, {'given': 'BOHUSLAV V.', 'family': 'KOKTA', 'sequence': 'additional', 'affiliation': [{'name': 'Centre de Recherche en Pâtes et Papiers, Université du Québec à Trois-Rivières, Trois-Rivières, Québec, Canada G9A 5H7'}]}], 'issued': {'date-parts': [[1982, 6, 18]]}, 'publisher': 'AMERICAN CHEMICAL SOCIETY', 'title': ['Graft Copolymerization of Lignosulfonate with Methacrylic Acid and Acrylate Monomers'], 'type': 'book-chapter'}, {'DOI': '10.1021/acs.biomac.7b00229.s001', 'issued': {'date-parts': [[None]]}, 'publisher': 'American Chemical Society (ACS)', 'title': ['Enzymatically Debranched Xylans in Graft Copolymerization'], 'type': 'component'}, {'DOI': '10.1021/acs.macromol.9b02646.s001', 'issued': {'date-parts': [[None]]}, 'publisher': 'American Chemical Society (ACS)', 'title': ['Toward More Sustainable Elastomers: Stereoselective Copolymerization of Linear Terpenes with Butadiene'], 'type': 'component'}, {'DOI': '10.1021/acscatal.1c04449.s001', 'issued': {'date-parts': [[None]]}, 'publisher': 'American Chemical Society (ACS)', 'title': ['Heterotrinuclear Ring Opening Copolymerization Catalysis: Structureactivity Relationships'], 'type': 'component'}]\n"
     ]
    }
   ],
   "source": [
    "from crossref.restful import Works\n",
    "import json\n",
    "\n",
    "works = Works()\n",
    "\n",
    "# Performing the search for sources on the topic of copolymerizations \n",
    "query_result = (\n",
    "    works.query(bibliographic=\"___\") # TODO define the search topic \n",
    "    .select(\"DOI\", \"title\", \"author\", \"type\", \"publisher\", \"issued\")\n",
    "    .sample(___) # TODO define the number of papers \n",
    ")\n",
    "\n",
    "results = [item for item in query_result]\n",
    "\n",
    "# Save results including their metadata in a json file\n",
    "with open(\"copolymerization_results.json\", \"w\") as file:\n",
    "    json.dump(results, file)\n",
    "\n",
    "# TODO print the results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-26T10:57:24.620494Z",
     "start_time": "2025-06-26T10:57:06.290648Z"
    }
   },
   "id": "87afacf487b0ffa9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next step is to download the relevant papers. There are multiple datasets and tools which can be used for data mining. While downloading papers, please always be aware of copyright.\n",
    "\n",
    "Since this step can take some time, already downloaded example papers can be found in the pdf folder. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10d24888e07aabf"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10.26434_chemrxiv-2024-1l0sn.pdf', '10.26434_chemrxiv-2024-rfzjm.pdf', '10.26434_chemrxiv-2021-2x06r-v3.pdf', '10.26434_chemrxiv-2024-tddfc-v2.pdf', '10.26434_chemrxiv.14217314.v1.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"pdfs\"\n",
    "files = os.listdir(folder_path)\n",
    "pdf_files = [f for f in files if f.lower().endswith('.pdf')]\n",
    "\n",
    "print(pdf_files)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-26T11:19:23.736567Z",
     "start_time": "2025-06-26T11:19:23.735132Z"
    }
   },
   "id": "1733ec06f988cd73"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Annotating a test and validation dataset\n",
    "\n",
    "To evaluate the data extraction and find the best hyperparameters one must have a test and validation set. Annotating at least a small part of the obtained article dataset is crucial. A reasonable number of annotated test and validation paper would be between 10-20 papers. The more diverse and representative the test and validation set of the whole paper corpus are, the better. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abc0fb3fbefdea5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# I will add some schema here"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c38b1591f4ef44c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Converting the PDF documents to text\n",
    "\n",
    "An important part of data extraction pipelines is often converting inputs into a form that the text-based pipelines can use.\n",
    "\n",
    "In many cases, this conversion involves that image inputs (e.g., scans of a paper) must be converted into text. To convert the PDF documents into text we will use a so-called OCR (Optical Character Recognition) tool. There is a variety of OCR tools available. We will now use the PyMuPDF python package since its easy to use and fast. There are also special packages made for the convertion of scientific publications like [NOUGAT](https://facebookresearch.github.io/nougat/) (Warning: NOUGAT is resource intensive and should be run on a GPU). "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb317faea9ff9057"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "pdf_folder = ___ # TODO: Specify the folder path where the PDF files are located\n",
    "\n",
    "output_folder = ___ # TODO: Specify the output folder path for the text files\n",
    "\n",
    "# Make sure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Loop through all PDF files in the folder\n",
    "for filename in os.listdir(pdf_folder):\n",
    "    if filename.lower().endswith(\".pdf\"):\n",
    "        file_path = os.path.join(pdf_folder, filename)\n",
    "        \n",
    "        # Open the PDF file\n",
    "        doc = fitz.open(file_path)\n",
    "        \n",
    "        # Extract text from all pages\n",
    "        full_text = \"\"\n",
    "        for page in doc:\n",
    "           # TODO Use .get_text() to extract the text from the documents and add it to full_text\n",
    "        \n",
    "        # TODO print the extracted text\n",
    "        doc.close()\n",
    "        \n",
    "        # TODO: Write the extracted text into a .txt file with the same name\n",
    "        txt_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "        txt_path = os.path.join(output_folder, txt_filename)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d639df2ea3397abe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cleaning the document\n",
    "\n",
    "Since most of the downloaded files include irrelevant sections like bibliography and acknowledgments the next step would be to remove those.\n",
    "\n",
    "The simplest method to remove irrelevant section would be using hard-coded rules. We will remove extraneous line breaks and irrelevant sections. Therefore, we wil use Regular expressions ([regex](https://docs.python.org/3/library/re.html)). They are a powerful tool for pattern matching, allowing for complex searches, substitutions, and data extraction based on specific string patterns."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "297d82949e517143"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_document(text):\n",
    "    \"\"\"\n",
    "    Cleans a single document's text.\n",
    "    - Keeps only the content between 'Introduction' and 'Acknowledgements'\n",
    "    - Removes line breaks within paragraphs\n",
    "    - Removes the heading 'Acknowledgements'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove double newlines to prepare for paragraph fixing\n",
    "    text = text.replace(\"\\n\\n\", \"\\n\")\n",
    "\n",
    "    # Extract the part between Introduction and Acknowledgements\n",
    "    pattern = re.compile(r\"Introduction.*?Acknowledgements\", re.DOTALL)\n",
    "    matches = re.findall(___, ___) # TODO: define the used regex pattern and the text to be cleaned \n",
    "\n",
    "    if matches:\n",
    "        filtered = matches[0].replace(\"Acknowledgements\", \"\")\n",
    "    else:\n",
    "        # If pattern not found, keep original text\n",
    "        # TODO: define the filtered text when no pattern is found \n",
    "    \n",
    "    return filtered"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6b7ac8f0979e5c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Replace this with the actual folder where the .txt files are saved\n",
    "text_folder = ___  \n",
    "\n",
    "# TODO: Replace this with the desired output folder for the cleaned text\n",
    "cleaned_folder = ___  \n",
    "os.makedirs(cleaned_folder, exist_ok=True)\n",
    "\n",
    "# Loop through all text files and clean them\n",
    "for filename in os.listdir(text_folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(text_folder, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Clean the content\n",
    "        cleaned =  # TODO: add the correct call of the cleaning function\n",
    "\n",
    "        # TODO: Print the cleaned content\n",
    "\n",
    "        \n",
    "        # Save the cleaned version\n",
    "        cleaned_path = os.path.join(cleaned_folder, filename)\n",
    "        with open(cleaned_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(cleaned)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20c879758fd3a2ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chunking the text\n",
    "\n",
    "Models always have a context window, which is the number of tokens ([short explanation for tokens](https://www.geeksforgeeks.org/nlp/tokenization-in-natural-language-processing-nlp/)) they can process at a given time.  \n",
    "This becomes a problem when we want to process long texts that exceed this limit.\n",
    "\n",
    "To handle this, we break the input into **smaller overlapping segments**, called **chunks**, that fit within the model's context window.\n",
    "\n",
    "We use a method called `RecursiveCharacterTextSplitter` from LangChain.  \n",
    "This splitter breaks the text at **logical boundaries** (like paragraphs, sentences, or characters) in a smart way, trying to preserve semantic structure rather than cutting arbitrarily.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85a4e8542c67e258"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# Function to chunk a given text into overlapping segments\n",
    "def chunk_text(text):\n",
    "    \"\"\"\n",
    "    Splits a given text into overlapping chunks using RecursiveCharacterTextSplitter.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=___,       # TODO: Define chunk size\n",
    "        chunk_overlap=___     # TODO: Define overlap size\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db995119972d4ffa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Specify folder containing the cleaned text files\n",
    "cleaned_folder = ___  \n",
    "\n",
    "for filename in os.listdir(cleaned_folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(cleaned_folder, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        chunks = # TODO: call the chunking function with the content\n",
    "\n",
    "        print(f\"{len(chunks)} chunks created for {filename}.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ca66516bad5809"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✅ Summary: What We've Learned So Far\n",
    "\n",
    "In this first part of the tutorial, you have gone through the essential preprocessing steps for scientific document extraction using LLMs:\n",
    "\n",
    "1. **Data collection**  \n",
    "   You used the Crossref API to collect metadata about relevant scientific articles and stored them in a structured JSON format.\n",
    "\n",
    "2. **PDF handling**  \n",
    "   You explored how to programmatically list and process PDF files using PyMuPDF (`fitz`) to extract raw text content.\n",
    "\n",
    "3. **Text cleaning**  \n",
    "   You implemented rule-based cleaning techniques using regular expressions to remove irrelevant sections such as bibliographies or acknowledgements.\n",
    "\n",
    "4. **Text chunking**  \n",
    "   You split cleaned texts into overlapping chunks to prepare them for LLM processing, taking context window limitations into account.\n",
    "\n",
    "These steps are foundational for building reliable data extraction workflows and ensure that the inputs are clean, structured, and manageable in size for downstream tasks like embedding, classification, or entity extraction.\n",
    "\n",
    "In the next section, you will use this structured and chunked data to **perform actual information extraction using LLMs**.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82985c47ac651412"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4534bf28e6c3698c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
