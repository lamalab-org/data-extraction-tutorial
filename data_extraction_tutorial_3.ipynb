{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing & Controlled Output\n",
    "\n",
    "Before performing large-scale extraction, it's important to **understand the structure and limitations** of the data and model outputs.\n",
    "\n",
    "This step is part of the **preprocessing workflow** and helps you identify:\n",
    "\n",
    "- What kind of information the model is able to extract reliably,\n",
    "- Which fields are ambiguous or frequently missing or incorrect,\n",
    "- And how to optimize and reduce common errors in automatic extraction.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75a5618f43633084"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ§© Constrained Output Formats\n",
    "\n",
    "One powerful strategy is to constrain the output format of the model â€“ for example, forcing it to respond in a fixed **JSON structure**.  \n",
    "This makes results easier to validate, parse, and store in downstream pipelines.\n",
    "\n",
    "To define and validate such structured outputs in Python, we use **[Pydantic](https://docs.pydantic.dev/latest/)** â€“ a robust library for data modeling and validation.\n",
    "\n",
    "Below, we define an example schema for extracting relevant data from polymer-related texts:\n",
    "\n",
    "- `name`: The name of the polymer  \n",
    "- `synthesis_method`: How the polymer was synthesized  \n",
    "- `synthesis_temperature`: The temperature which was used to synthesize the polymer  \n",
    "- `homopolymer`: Boolean indicating whether the polymer is a homopolymer or not  \n",
    "\n",
    "We will prompt the model to return its answer strictly in this format and validate it using the Pydantic schema.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5c953d23a00d726"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional, List\n",
    "\n",
    "class PolymerExtraction(BaseModel):\n",
    "    name: str\n",
    "    synthesis_method: Optional[str] = None\n",
    "    temperature: Optional[List[float]] = None\n",
    "    homopolymer: Optional[bool] = None\n",
    "    \n",
    "class PolymerList(BaseModel):\n",
    "    polymers: List[PolymerExtraction]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f1a7ab287fe1f77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import instructor\n",
    "from litellm import OpenAI\n",
    "\n",
    "# Patch OpenAI client to support structured output via response_model\n",
    "client = instructor.patch(OpenAI(), mode=instructor.Mode.MD_JSON)\n",
    "\n",
    "# Load input text (e.g. from a scientific article)\n",
    "with open(\"example_paper.txt\", \"r\") as f:\n",
    "     polymer_text = f.read()\n",
    "\n",
    "# Define user prompt with short instruction + input text\n",
    "user_prompt = f\"\"\"Extract polymer data (name, synthesis method, temperature, homopolymer) from the following text:\n",
    "{polymer_text}\"\"\"\n",
    "\n",
    "# Call the LLM and request structured output\n",
    "extracted_data = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a polymer chemistry expert extracting all data about polymers from \"\n",
    "                \"scientific publications.\"\n",
    "            )\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    \n",
    "    # This is where the Pydantic schema is used to define the expected output\n",
    "    response_model=PolymerList,\n",
    "\n",
    "    # Retry up to 2 times if the output is invalid or not parseable\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "# The result is already validated and returned as a PolymerList object\n",
    "print(extracted_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2b3fe861b5ebf11"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluations\n",
    "\n",
    "Every extraction task needs a good way to evaluate whether the extracted data is correct and give it a score of how correct it is. The goal is to quantify the extraction pipelineâ€™s (modelâ€™s) performance. With partial scores giving insight on how correct a data point is, usually between 0 and 1, the pipeline can be improved by fixing any edge cases or errors found by comparing lower scored data points.\n",
    "\n",
    "To assess how well the LLM is performing, we need to compare its output against ground truth annotations.\n",
    "\n",
    "A common starting point is to compute basic metrics like **Precision** and **Recall**:\n",
    "\n",
    "- **Precision**: How many of the extracted items are correct?\n",
    "- **Recall**: How many of the correct items were actually extracted?\n",
    "\n",
    "In a first step, we will evaluate based on **exact string matches** without deeper matching logic.\n",
    "Then we show a minimal example of how to include matching logic (e.g. fuzzy name similarity).\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e711fc9404ef52"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.50\n",
      "Recall: 0.67\n",
      "F1-Score: 0.57\n"
     ]
    }
   ],
   "source": [
    "# Ground truth (annotated polymers)\n",
    "ground_truth = [\"polyethylene\", \"polystyrene\", \"poly(lactic acid)\"]\n",
    "\n",
    "# Model output (extracted polymers)\n",
    "extracted = [\"polyethylene\", \"polymer\", \"polymer X\", \"poly(lactic acid)\"]\n",
    "\n",
    "# Compute simple metrics\n",
    "true_positives = len(set(ground_truth) & set(extracted))\n",
    "\n",
    "precision = true_positives / (len(extracted) + 1e-8)\n",
    "recall = true_positives / (len(ground_truth) + 1e-8)\n",
    "f1_score = 2 / ((1 / recall) + (1 / precision))\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1_score:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-30T11:54:47.456193Z",
     "start_time": "2025-06-30T11:54:47.449758Z"
    }
   },
   "id": "95c5e8c588e0e2a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Feel free to modify the `ground_truth` or `extracted` lists above to explore how different prediction results affect the evaluation metrics."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db537a159b015722"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Matching and Fuzzy Comparison\n",
    "\n",
    "In real-world evaluations, we typically compare the modelâ€™s predicted outputs to a set of manually annotated ground truth entries.\n",
    "\n",
    "However, this is not just a matter of counting:  \n",
    "- The model may return **more items than expected** (hallucinations),\n",
    "- **Miss some relevant entries** (false negatives),\n",
    "- Or express correct answers in **slightly different formats** (e.g. synonyms, abbreviations, typos).\n",
    "\n",
    "To handle this, we need a way to **map predicted entries to ground truth items** before we compute metrics like precision and recall.  \n",
    "This step is called **matching**.\n",
    "\n",
    "A common first approach is **fuzzy string matching**, where we match two entries if they are similar above a certain threshold (instead of requiring exact equality).\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7322a0bd19ddd7af"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched pairs (threshold â‰¥ 0.8):\n",
      "\n",
      "âœ“ 'polyethylene' â†” 'polyethylene'\n",
      "âœ“ 'poly(lactic acid)' â†” 'poly(lactic acid)'\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Define a fuzzy matching function using character similarity\n",
    "def fuzzy_match(a, b, threshold=___):  # TODO: Try out different similarity thresholds\n",
    "    \"\"\"\n",
    "    Returns True if strings `a` and `b` are similar above a certain threshold.\n",
    "    Uses SequenceMatcher for character-based similarity.\n",
    "    \"\"\"\n",
    "    ratio = SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "    return ratio >= threshold \n",
    "\n",
    "# Ground truth annotations (gold standard)\n",
    "ground_truth = [\"polyethylene\", \"polystyrene\", \"poly(lactic acid)\"]\n",
    "\n",
    "# Model output (predictions)\n",
    "predicted = [\"polyethylene\", \"polymer\", \"poly(lactic acid)\", \"poly(lactic-acid)\", \"polylactic acid\"]\n",
    "\n",
    "\n",
    "print(\"Matched pairs (fuzzy threshold â‰¥ ___):\\n\")  # TODO: Fill in threshold value in message\n",
    "\n",
    "# Try to find matching pairs based on fuzzy similarity\n",
    "for gt in ground_truth:\n",
    "    for pred in predicted:\n",
    "        if fuzzy_match(gt, pred, threshold=0.8):  # <-- try different thresholds!\n",
    "            # TODO: Print the match pair\n",
    "            print(f\"âœ“ '{___}' â†” '{___}'\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-30T12:01:34.113405Z",
     "start_time": "2025-06-30T12:01:34.095313Z"
    }
   },
   "id": "a2569ac5f99c8103"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In more complex schemas involving multiple fields, lists, or nested structures, exact string matching is usually not sufficient.\n",
    "\n",
    "Small variations in how information is expressed (e.g. formatting or synonyms) can lead to incorrect evaluation results.\n",
    "\n",
    "To obtain **meaningful metrics**, it is essential to match the extracted entries to the ground truth before calculating metrics. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfcbacb770c2620f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalize Before Matching\n",
    "\n",
    "Before comparing values between model predictions and ground truth annotations, it is crucial to **normalize all units and formats**.  \n",
    "\n",
    "Otherwise, two semantically equivalent values like `22 g` and `22000 mg` would be seen as a mismatch.\n",
    "\n",
    "This applies to:\n",
    "- **Physical quantities** like temperature, mass, or pressure\n",
    "- **Chemical names or formulas**, which can be normalized to canonical representations (e.g. **SMILES** strings)\n",
    "\n",
    "Below is an example of how to normalize numerical units using the [`pint`](https://pint.readthedocs.io/en/stable/) library.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f51173d293a808"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 22000.0 mg\n",
      "to 22.0 gram\n"
     ]
    }
   ],
   "source": [
    "from pint import UnitRegistry\n",
    "\n",
    "# Ground truth annotation\n",
    "truth = {\"mass\": {\"value\": 22.0, \"unit\": \"g\"}}\n",
    "\n",
    "# Model prediction in different unit (but same value!)\n",
    "prediction = {\"mass\": {\"value\": 22000.0, \"unit\": \"mg\"}}\n",
    "\n",
    "# Initialize unit registry for physical quantities\n",
    "ureg = UnitRegistry()\n",
    "\n",
    "# Create a string representation: e.g. \"22000.0 mg\"\n",
    "text_representation_of_value = (\n",
    "    str(prediction[\"mass\"][\"value\"]) + \" \" + prediction[\"mass\"][\"unit\"]\n",
    ")\n",
    "\n",
    "print(\"Converting:\", text_representation_of_value)\n",
    "\n",
    "# Convert the predicted value to grams (g)\n",
    "normalized_pint_quantity = ureg(___).to(\"g\") # TODO: add the value to be converted\n",
    "\n",
    "print(\"â†’\", normalized_pint_quantity)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-30T12:10:37.015185Z",
     "start_time": "2025-06-30T12:10:36.663053Z"
    }
   },
   "id": "5541b623187d2fa2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we focused on the **postprocessing and evaluation** of structured outputs from LLM-based extraction systems:\n",
    "\n",
    "- We used **constrained decoding** to extract data in a predefined JSON format.\n",
    "- We defined a **Pydantic schema** to validate and structure the modelâ€™s output.\n",
    "- We normalized units (e.g. mg â†’ g, K â†’ Â°C) to allow fair comparisons.\n",
    "- We introduced **basic evaluation metrics** like precision, recall, and F1-score.\n",
    "- Finally, we explored the importance of **fuzzy matching** to account for variations in wording, units, and structure.\n",
    "\n",
    "These steps form the basis for building **trustworthy and reproducible extraction pipelines** for scientific applications.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a070b04065b5ec9d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
